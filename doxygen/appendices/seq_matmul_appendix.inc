This class differentiates on the length of a sequence to be multiplied. Short sequences below a - currently fixed -
threshold #MATMUL_LONG_THRESHOLD are computed by a different set of shaders than longer sequences. In the
following we describe how these shaders work and what memory layout they are assuming / returning.

The inputs for long and short sequences are the same, namely a 2D texture that has a width equivalent to the
embedding dimension (div 4) and one row per token.

For example, to represent a tensor of size 3x4096, the input texture would look like:

@code
+---------------------------------------------------------------------+
| T0(0)  ................................................... T0(1023) |
+---------------------------------------------------------------------+
| T1(0)  ................................................... T1(1023) |
+---------------------------------------------------------------------+
| T2(0)  ................................................... T2(1023) |
+---------------------------------------------------------------------+
@endcode

The output texture will have the same format, with the exception that the width of the rows
might differ, depending on the size of the weight matrix.
A popular use case for this kind of multiplication is multi-headed self-attention, which will
yield a substructured resulting texture that further splits up each row in the resulting matrix
into column-blocks that relate to the "heads" of the multi-head attention function, for
example:

@code
+-------------+-------------+-----------------------+-----------------+
| T0(0) head0 | T0(m) head1 | ......................| T0(1023) head n |
+-------------+-------------+-----------------------+-----------------+
| T1(0) head0 | T0(m) head1 | ......................| T0(1023) head n |
+-------------+-------------+-----------------------+-----------------+
|     ...     |     ...     |         ...           |      ...        |
+-------------+-------------+-----------------------+-----------------+
| Tk(0) head0 | Tk(m) head1 | ......................| Tk(1023) head n |
+-------------+-------------+-----------------------+-----------------+
@endcode

\par Quantized Weights
The values of the weight matrix which is the RHS of the multiplication are given as 4-bit
quantized data where we store the weight matrix on column-major order inside a 32-bit (u)integer
RGBA texture, a total of 32 weights per RGBA pixel. In order to dequantize, two additional
textures are used, one for scaling the 4-bit quantized values to floating point and one
zero-point (offset) texture that shifts the quantization bias to allow for negative values. This
is also known an <i>affine quantization</i>.
The output generated by this function has the same format as the input, it might differ in the
width of the rows (in case the matrix multiplication here is not done with a quadratic matrix).

\par Long Sequences (quantized)

For long sequences, the vertex shader will perform the fetching and dequantization of the weight matrix elements
and use GLSL interface variables to move them to the fragment shader. Instanced rendering and the ROPs are used
to accumulate the results of the matrix multiplication into the resulting texture. In this case, the rendering
is done column-wise using lines as proxy primitives, essentially carrying out dot-product operations in
segments.

\par Short Sequences (quantized)
Short sequences occur quite often during the autoregressive prediction of single tokens in sequence learning.
For this reason, we employ a shader that is optimized for single rows (the left matrix degenerated to a vector).
In contrast to the long sequence multiplication, the short multiplication uses horizontal lines as proxies for
accumulating the result of the underlying dot product operations. Each line corresponds to one  row of the
output matrix and the rendering is done using instancing. The instances define an offset within the
segmented dot-product that is the base of the matrix multiplication that is carried out here.
The data fetch for the right (weight) matrix happens in the fragment shader, as does the dequantization.
